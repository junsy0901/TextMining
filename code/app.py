### main.pyë¥¼ ì›¹ì—ì„œ ì‹¤í–‰ì‹œí‚¤ê¸° ìœ„í•´
### streamlit uië¥¼ ì¶”ê°€í•œ ì½”ë“œ

import streamlit as st
import torch
import pandas as pd
from sentence_transformers import SentenceTransformer, util
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_community.chat_models import ChatOllama

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ê³„ì•½ì„œ ì¡°í•­ ë¶„ì„ê¸°",
    page_icon="ğŸ“‹",
    layout="wide"
)

st.title("ğŸ“‹ ê³„ì•½ì„œ ì¡°í•­ ìœ ë¶ˆë¦¬ ë¶„ì„ê¸°")
st.markdown("---")

@st.cache_resource
def load_models():
    """ëª¨ë¸ë“¤ì„ ë¡œë“œí•˜ê³  ìºì‹±"""
    # ìœ ë¶ˆë¦¬ íŒë‹¨ ëª¨ë¸
    classification_dir = "../model/classification"
    classifier_model = AutoModelForSequenceClassification.from_pretrained(classification_dir)
    classifier_tokenizer = AutoTokenizer.from_pretrained(classification_dir)
    
    # ì˜ë¯¸ ì„ë² ë”© ëª¨ë¸
    semantic_dir = "../model/legal-kr-sbert-contrastive"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    semantic_model = SentenceTransformer(semantic_dir).to(device)
    
    return classifier_model, classifier_tokenizer, semantic_model, device

@st.cache_data
def load_data():
    """ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ìºì‹±"""
    dataset = torch.load(
        "embedding_dataset.pt",
        map_location=lambda storage, loc: storage.cuda() if torch.cuda.is_available() else storage
    )
    
    df = pd.read_csv("labeled.csv")
    
    return dataset, df

# ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë“œ
with st.spinner("ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘..."):
    classifier_model, classifier_tokenizer, semantic_model, device = load_models()
    dataset, df = load_data()
    
    texts = dataset["texts"]
    embeddings = dataset["embeddings"].to(device)

st.success(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Device: {device})")

def predict_unfairness(clauses):
    """ì¡°í•­ì˜ ìœ ë¶ˆë¦¬ë¥¼ ì˜ˆì¸¡"""
    inputs = classifier_tokenizer(clauses, padding=True, truncation=True, return_tensors="pt")
    outputs = classifier_model(**inputs)
    preds = torch.argmax(outputs.logits, dim=1)
    return preds.tolist()  # 0: ìœ ë¦¬, 1: ë¶ˆë¦¬

def get_similar_clauses(query, top_k=5):
    """ìœ ì‚¬í•œ ì¡°í•­ë“¤ì„ ê²€ìƒ‰"""
    query_emb = semantic_model.encode(query, convert_to_tensor=True)
    cos_scores = util.pytorch_cos_sim(query_emb, embeddings)[0]
    top_results = torch.topk(cos_scores, k=top_k)

    results = []
    for idx in top_results.indices:
        clause = texts[idx]
        # labeled.csvì—ì„œ basis ì°¾ê¸°
        matched = df[df["text"] == clause]
        basis = matched["basis"].values[0] if not matched.empty else ""
        similarity_score = cos_scores[idx].item()
        results.append({
            "clause": clause, 
            "basis": basis,
            "similarity": similarity_score
        })
    
    return results

@st.cache_resource
def setup_llm():
    """LLM ì²´ì¸ ì„¤ì •"""
    llm = ChatOllama(model="anpigon/EEVE-Korean-10.8B:latest")
    
    prompt_template = PromptTemplate(
        input_variables=["clause", "similar"],
        template="""ë‹¤ìŒì€ ì„œë¹„ìŠ¤ ì•½ê´€ì˜ ì¡°í•­ì…ë‹ˆë‹¤:

ì¡°í•­:
{clause}

ìœ ì‚¬í•œ ì¡°í•­ë“¤:
{similar}

ì´ ì¡°í•­ì´ ì™œ ë¶ˆë¦¬í•œì§€ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
"""
    )
    
    return LLMChain(llm=llm, prompt=prompt_template)

llm_chain = setup_llm()

def generate_explanation(clause, similar_clauses):
    """ì¡°í•­ì— ëŒ€í•œ ì„¤ëª… ìƒì„±"""
    similar_text = "\n\n".join(
        [f"- ì¡°í•­:\n{item['clause']}\nì„¤ëª…:\n{item.get('basis', '')}" for item in similar_clauses]
    )

    return llm_chain.run({
        "clause": clause,
        "similar": similar_text
    })

# UI êµ¬ì„±
st.markdown("## ğŸ“ ì¡°í•­ ì…ë ¥")

# ì…ë ¥ ë°©ì‹ ì„ íƒ
input_method = st.radio(
    "ì…ë ¥ ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”:",
    ["ë‹¨ì¼ ì¡°í•­", "ì—¬ëŸ¬ ì¡°í•­ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)"]
)

if input_method == "ë‹¨ì¼ ì¡°í•­":
    clause_input = st.text_area(
        "ë¶„ì„í•  ì¡°í•­ì„ ì…ë ¥í•˜ì„¸ìš”:",
        height=100,
        placeholder="ì˜ˆ: íšŒì‚¬ëŠ” ì–¸ì œë“ ì§€ ì‚¬ì „ í†µì§€ ì—†ì´ ì„œë¹„ìŠ¤ë¥¼ ì¤‘ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )
    input_clauses = [clause_input] if clause_input.strip() else []
else:
    multi_clause_input = st.text_area(
        "ë¶„ì„í•  ì¡°í•­ë“¤ì„ ì…ë ¥í•˜ì„¸ìš” (ê° ì¤„ë§ˆë‹¤ í•˜ë‚˜ì”©):",
        height=200,
        placeholder="ì¡°í•­ 1\nì¡°í•­ 2\nì¡°í•­ 3"
    )
    input_clauses = [line.strip() for line in multi_clause_input.split('\n') if line.strip()]

# ë¶„ì„ ì„¤ì •
col1, col2 = st.columns(2)
with col1:
    top_k = st.slider("ìœ ì‚¬ ì¡°í•­ ê²€ìƒ‰ ê°œìˆ˜", min_value=1, max_value=10, value=5)
with col2:
    min_similarity = st.slider("ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’", min_value=0.0, max_value=1.0, value=0.5)

# ë¶„ì„ ì‹¤í–‰
if st.button("ğŸ” ì¡°í•­ ë¶„ì„ ì‹œì‘", type="primary"):
    if not input_clauses:
        st.warning("ë¶„ì„í•  ì¡°í•­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ì¡°í•­ì„ ë¶„ì„í•˜ëŠ” ì¤‘..."):
            labels = predict_unfairness(input_clauses)
            
            st.markdown("## ğŸ“Š ë¶„ì„ ê²°ê³¼")
            
            for i, (clause, label) in enumerate(zip(input_clauses, labels), 1):
                with st.expander(f"ì¡°í•­ {i}: {'ğŸ”´ ë¶ˆë¦¬' if label == 1 else 'ğŸŸ¢ ìœ ë¦¬'}", expanded=True):
                    st.markdown(f"**ì¡°í•­ ë‚´ìš©:**")
                    st.info(clause)
                    
                    if label == 1:  # ë¶ˆë¦¬í•œ ì¡°í•­
                        st.markdown("**ğŸ” ìœ ì‚¬ ì¡°í•­ ë¶„ì„:**")
                        
                        similar = get_similar_clauses(clause, top_k=top_k)
                        # ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§
                        similar = [s for s in similar if s['similarity'] >= min_similarity]
                        
                        if similar:
                            # ìœ ì‚¬ ì¡°í•­ í‘œì‹œ
                            for j, sim_clause in enumerate(similar, 1):
                                with st.container():
                                    st.markdown(f"**ìœ ì‚¬ ì¡°í•­ {j} (ìœ ì‚¬ë„: {sim_clause['similarity']:.3f})**")
                                    st.text(sim_clause['clause'])
                                    if sim_clause['basis']:
                                        st.markdown("**ê·¼ê±°:**")
                                        st.text(sim_clause['basis'])
                                    st.markdown("---")
                            
                            # AI ì„¤ëª… ìƒì„±
                            st.markdown("**ğŸ§  AI ë¶„ì„ ì„¤ëª…:**")
                            with st.spinner("AIê°€ ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                                try:
                                    explanation = generate_explanation(clause, similar)
                                    st.markdown(explanation)
                                except Exception as e:
                                    st.error(f"ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                        else:
                            st.warning(f"ìœ ì‚¬ë„ {min_similarity} ì´ìƒì¸ ì¡°í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
                    else:  # ìœ ë¦¬í•œ ì¡°í•­
                        st.success("âœ… ì´ ì¡°í•­ì€ ìœ ë¦¬í•œ ê²ƒìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ì‚¬ì´ë“œë°” ì •ë³´
with st.sidebar:
    st.markdown("## â„¹ï¸ ëª¨ë¸ ì •ë³´")
    st.markdown(f"""
    - **ë¶„ë¥˜ ëª¨ë¸**: ì¡°í•­ ìœ ë¶ˆë¦¬ íŒë‹¨
    - **ì„ë² ë”© ëª¨ë¸**: í•œêµ­ì–´ ë²•ë¥  SBERT
    - **ìƒì„± ëª¨ë¸**: EEVE-Korean-10.8B
    - **ì²˜ë¦¬ ì¥ì¹˜**: {device}
    - **ë°ì´í„°ì…‹ í¬ê¸°**: {len(texts):,}ê°œ ì¡°í•­
    """)
    
    st.markdown("## ğŸ”§ ì‚¬ìš© ë°©ë²•")
    st.markdown("""
    1. ë¶„ì„í•  ê³„ì•½ì„œ ì¡°í•­ì„ ì…ë ¥
    2. ë¶„ì„ ì„¤ì • ì¡°ì • (ì„ íƒì‚¬í•­)
    3. 'ì¡°í•­ ë¶„ì„ ì‹œì‘' ë²„íŠ¼ í´ë¦­
    4. ê²°ê³¼ í™•ì¸ ë° ìœ ì‚¬ ì¡°í•­ ê²€í† 
    """)
    
    st.markdown("## âš ï¸ ì£¼ì˜ì‚¬í•­")
    st.markdown("""
    - AI ë¶„ì„ ê²°ê³¼ëŠ” ì°¸ê³ ìš©ì…ë‹ˆë‹¤
    - ë²•ì  ì¡°ì–¸ì„ ëŒ€ì²´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
    - ì „ë¬¸ê°€ ê²€í† ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤
    """)
