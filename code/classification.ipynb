{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e26a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, json, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 파일 읽기\n",
    "records = []\n",
    "for path in glob.glob(\"../data/*/*.json\"):\n",
    "    j = json.load(open(path, encoding=\"utf-8\"))\n",
    "    text = \"\\n\".join(j.get(\"clauseArticle\", []))\n",
    "    label = int(j[\"dvAntageous\"]) - 1    # 예: 1→0, 2→1\n",
    "    records.append({\"text\": text, \"label\": label})\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "val_df, test_df  = train_test_split(test_df, test_size=0.5, stratify=test_df[\"label\"], random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b9513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 6400/6400 [00:00<00:00, 8092.24 examples/s] \n",
      "Map: 100%|██████████| 800/800 [00:00<00:00, 10991.97 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9600' max='9600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9600/9600 36:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.393700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.173200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.127400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.117700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.040300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.019300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.060600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.010900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.009300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 800/800 [00:00<00:00, 7098.28 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.08260364830493927,\n",
       " 'eval_accuracy': 0.98875,\n",
       " 'eval_runtime': 42.2248,\n",
       " 'eval_samples_per_second': 18.946,\n",
       " 'eval_steps_per_second': 0.592,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "# 1) 토크나이저 & 모델 로드\n",
    "MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=2\n",
    ")\n",
    "\n",
    "# 2) Huggingface Dataset으로 변환\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds   = Dataset.from_pandas(val_df)\n",
    "\n",
    "# 3) 토크나이징 함수\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"], \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True)\n",
    "val_tok   = val_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "# 4) DataCollator (dynamic padding)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# 5) 평가 지표 정의 (정확도)\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "# 6) Trainer 세팅\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    do_eval=True,          # old-style evaluation\n",
    "    eval_steps=500,        # 원하는 스텝마다\n",
    "    logging_steps=500,     # 로깅 스텝\n",
    "    save_steps=500         # 체크포인트 저장 스텝\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 7) 학습 실행\n",
    "trainer.train()\n",
    "\n",
    "# 8) 테스트 평가\n",
    "trainer.evaluate(Dataset.from_pandas(test_df).map(tokenize_fn, batched=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f2567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('classification_model\\\\tokenizer_config.json',\n",
       " 'classification_model\\\\special_tokens_map.json',\n",
       " 'classification_model\\\\vocab.txt',\n",
       " 'classification_model\\\\added_tokens.json',\n",
       " 'classification_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"classification\")  \n",
    "tokenizer.save_pretrained(\"classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "443ec508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"제3조(보험금의 지급사유)\n",
      "회사는 피보험자에게 다음 사...\" → 유리\n",
      "\"제2항 \n",
      "신탁계약에 의거 갑은 목적물에 대한 매도자의 ...\" → 불리\n",
      "\"을(수탁자)은 본 계약 이행 중 발생한 손해에 대하여 ...\" → 불리\n",
      "\"을은 본 계약과 관련하여 회사(갑)의 고의·중과실을 포...\" → 불리\n",
      "\"당사자 일방은 상대방에게 본 계약 해지 의사를 서면(전...\" → 불리\n",
      "\"회사는 피보험자에게 다음 중 어느 하나의 사유가 발생한...\" → 유리\n",
      "\"제2조(적용원칙) \n",
      "을이 주식의 위탁판매업무를 수행함에...\" → 유리\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 1) 저장된 체크포인트 경로 지정\n",
    "checkpoint_dir = \"classification\"  # Trainer 가 마지막에 저장한 폴더\n",
    "\n",
    "# 2) 토크나이저 & 모델 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_dir)\n",
    "model.eval()  # 평가 모드로 전환\n",
    "\n",
    "# 3) 분류 함수 정의\n",
    "def classify_clause(text: str) -> str:\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    pred_id = logits.argmax(dim=-1).item()\n",
    "    # 레이블 맵핑 (0: 불리, 1: 유리)\n",
    "    label_map = {1: \"불리\", 0: \"유리\"}\n",
    "    return label_map[pred_id]\n",
    "\n",
    "# 4) 예시 문장들로 테스트\n",
    "clauses = [\n",
    "    \"제3조(보험금의 지급사유)\\n회사는 피보험자에게 다음 사항 중 어느 하나의 사유가 발생한 경우에는 보험수익자에게 약정한 보험금((별표 1) 보험금 지급기준표 참조)을 지급한다.\\n1. 연금개시 전 보험기간 중 장해분류표(별표 3 참조) 중 동일한 재해를 원인으로 여러 신 체부위의 장해지급률을 더하여 80퍼센트이상인 장해상태가 되었을 때(최초 1회한) 고도재 해장해보험금\",\n",
    "    \"제2항 \\n신탁계약에 의거 갑은 목적물에 대한 매도자의 지위를 가지는 자로서 신탁재산 및 신탁계약의 업무범위 내에서만 책임을 부담하며, 이 분양계약에 의하여 갑에게 발생하는 일체의 의무(해약금반환, 입주지연시 지체상금, 하자보수 등)는 신탁계약 위탁사인 정이 부담하기로 한다.\\n갑  사업자(수탁자, 매도인)\\n을  고객(매수인)\\n정  제3자(위탁자)\\n\",\n",
    "    \"을(수탁자)은 본 계약 이행 중 발생한 손해에 대하여 회사(갑)의 과실이 없음을 증명하더라도, 발생 금액의 30%까지 책임을 부담한다.\",\n",
    "    \"을은 본 계약과 관련하여 회사(갑)의 고의·중과실을 포함한 모든 귀책 사유에 대하여 어떠한 손해배상 청구도 하지 않으며, 발생 가능한 모든 손해를 스스로 부담한다.\",\n",
    "    \"당사자 일방은 상대방에게 본 계약 해지 의사를 서면(전자문서 포함)으로 통지하고, 통지일로부터 30일이 경과하면 본 계약은 해지된다.\",\n",
    "    \"회사는 피보험자에게 다음 중 어느 하나의 사유가 발생한 경우에는 보험수익자에게 약정한 보험금을 지급합니다.\\n1. 보험기간 중에 상해의 직접결과로써 사망한 경우(질병으로 인한 사망은 제외합니다). \",\n",
    "    \"제2조(적용원칙) \\n을이 주식의 위탁판매업무를 수행함에 있어서는 관련법령에 위배되지 않는 한 본 계약이 적용되며, 관련법령이 변경되거나 갑의 정관이 변경되는 경우에는 갑 과 을이 합의하여 본 계약을 변경할 수 있다.\"\n",
    "]\n",
    "\n",
    "for c in clauses:\n",
    "    print(f\"\\\"{c[:30]}...\\\" → {classify_clause(c)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25b9b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
