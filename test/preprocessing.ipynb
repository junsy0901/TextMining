{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce36ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d81e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "def load_all_clauses_from_dir(root_dir):\n",
    "    records = []\n",
    "    for label in (\"유리\", \"불리\"):\n",
    "        folder = os.path.join(root_dir, label)\n",
    "        for fname in os.listdir(folder):\n",
    "            if not fname.endswith(\".json\"):\n",
    "                continue\n",
    "            path = os.path.join(folder, fname)\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            text = \" \".join(data.get(\"clauseArticle\", []))\n",
    "            basis = \" \".join(data.get(\"illdcssBasiss\", [])) if label == \"불리\" else None\n",
    "            records.append({\n",
    "                \"filename\": fname,\n",
    "                \"label\": label,\n",
    "                \"text\": text,\n",
    "                \"basis\": basis\n",
    "            })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "df = load_all_clauses_from_dir(\"../data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c27d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('labled.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4adf1a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 언어 모델 불러오기\n",
    "import torch\n",
    "from transformers import BertModel, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"skt/kobert-base-v1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "model = BertModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6b7346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Documents\\JSY\\KW\\3-1\\TextMining\\TextMining\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (3, 768)\n",
      "First embedding vector (truncated): [-0.14241247  0.13353735 -0.12907091 -0.17164774 -0.48322865]...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def embed_texts(\n",
    "    texts,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    batch_size: int = 16,\n",
    "    max_length: int = None,\n",
    "    device: torch.device = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Embed a list of texts into vector representations using the [CLS] token embedding.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): Input texts to embed.\n",
    "        model (torch.nn.Module): Pretrained Transformer model.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): Corresponding tokenizer.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        max_length (int, optional): Maximum token length. Defaults to model.config.max_position_embeddings.\n",
    "        device (torch.device, optional): Device for inference. Defaults to CUDA if available else CPU.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (len(texts), hidden_size) with embeddings.\n",
    "    \"\"\"\n",
    "    # Determine max_length if not provided\n",
    "    if max_length is None:\n",
    "        max_length = model.config.max_position_embeddings\n",
    "\n",
    "    # Determine device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Prepare model\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_embeds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i : i + batch_size]\n",
    "            encoded = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "            outputs = model(**encoded)\n",
    "            # Extract [CLS] token embedding (first token)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            all_embeds.append(cls_embeddings.cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_embeds)\n",
    "\n",
    "\n",
    "# ===================== Usage Example =====================\n",
    "if __name__ == \"__main__\":\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    # Sample texts\n",
    "    texts = [\n",
    "        \"Hello world!\",\n",
    "        \"This is a test sentence to embed.\",\n",
    "        \"여러 문장을 한 번에 임베딩해 봅니다.\"\n",
    "    ]\n",
    "\n",
    "    # Generate embeddings\n",
    "    embeddings = embed_texts(\n",
    "        texts=texts,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=2\n",
    "    )\n",
    "\n",
    "    # Display shapes and a sample\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")  # (3, hidden_size)\n",
    "    print(f\"First embedding vector (truncated): {embeddings[0][:5]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc3e8c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS 인덱스\n",
    "def build_faiss_index(embeddings: np.ndarray) -> faiss.Index:\n",
    "    \"\"\"\n",
    "    embeddings: (N, D) float32 numpy array\n",
    "    returns: FAISS index for cosine similarity\n",
    "    \"\"\"\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(embeddings)\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a76228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset max seq_len = 993 (at idx 6339)\n",
      "model.max_position_embeddings = 512\n"
     ]
    }
   ],
   "source": [
    "# 모델이 지원하는 최대 위치 임베딩 길이\n",
    "# 왜 뻑나는지 확인\n",
    "max_len = model.config.max_position_embeddings  \n",
    "max_seq = 0\n",
    "max_idx = None\n",
    "\n",
    "for i, text in enumerate(df[\"text\"].fillna(\"\").astype(str)):\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", truncation=False, padding=False)\n",
    "    seq_len = enc[\"input_ids\"].size(1)\n",
    "    if seq_len > max_seq:\n",
    "        max_seq, max_idx = seq_len, i\n",
    "\n",
    "print(f\"dataset max seq_len = {max_seq} (at idx {max_idx})\")\n",
    "print(f\"model.max_position_embeddings = {max_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892d2cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 구축\n",
    "index = build_faiss_index(embeddings)\n",
    "print(\"FAISS index total vectors:\", index.ntotal)\n",
    "\n",
    "# 첫 번째 항목으로 Top-5 검색\n",
    "D, I = index.search(embeddings[:1], k=5)\n",
    "for rank, idx in enumerate(I[0]):\n",
    "    sim   = D[0][rank]\n",
    "    label = df.loc[idx, \"label\"]\n",
    "    text  = df.loc[idx, \"text\"][:50] + \"…\"\n",
    "    basis = df.loc[idx, \"basis\"][:50] + \"…\" if df.loc[idx, \"basis\"] else \"\"\n",
    "    print(f\"Rank {rank+1}: (label={label}, sim={sim:.4f})\")\n",
    "    print(f\"  text : {text}\")\n",
    "    if basis:\n",
    "        print(f\"  basis: {basis}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca505f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m sbert = SentenceTransformer(\u001b[33m\"\u001b[39m\u001b[33mall-MiniLM-L6-v2\u001b[39m\u001b[33m\"\u001b[39m, device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 텍스트 리스트 준비\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m texts = \u001b[43mdf\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m].fillna(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).astype(\u001b[38;5;28mstr\u001b[39m).tolist()\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 배치 사이즈 32로, 진행바 띄워가며 임베딩 생성\u001b[39;00m\n\u001b[32m     11\u001b[39m embeddings = sbert.encode(\n\u001b[32m     12\u001b[39m     texts,\n\u001b[32m     13\u001b[39m     batch_size=\u001b[32m32\u001b[39m,\n\u001b[32m     14\u001b[39m     show_progress_bar=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     15\u001b[39m     convert_to_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# 뻑나는 함수 대신 다른 모델 가져와서 임베딩함\n",
    "\n",
    "# 가벼운 CPU용 모델로 불러오기\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "\n",
    "# 텍스트 리스트 준비\n",
    "texts = df[\"text\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "# 배치 사이즈 32로, 진행바 띄워가며 임베딩 생성\n",
    "embeddings = sbert.encode(\n",
    "    texts,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "# float32 변환 후 DataFrame에 추가\n",
    "df[\"embedding\"] = embeddings.astype(\"float32\").tolist()\n",
    "\n",
    "print(\"임베딩 shape:\", embeddings.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
